{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "oTE_70mxSinL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWKr0TihSaEc",
        "outputId": "27f9940e-cb80-4ded-be75-539ef80be3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: In the picture, a person is cleaning a hospital room. They are wearing blue scrubs and yellow gloves, using a mop to clean the floor. A hospital bed is in the room, along with a cleaning cart labeled \"CAUTION.\" The setting appears to be a clinical environment, indicating an effort to maintain cleanliness and hygiene.\n"
          ]
        }
      ],
      "source": [
        "def encode_image(image_path: str) -> str:\n",
        "    \"\"\"Encode image to base64 string for OpenAI API.\"\"\"\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "def multimodal_agent(image_path: str, text_input: str) -> str:\n",
        "    \"\"\"\n",
        "    A multimodal agent using OpenAI GPT-4V API.\n",
        "    Takes image + text and returns model output.\n",
        "    \"\"\"\n",
        "    # Fetch API key from Colab secrets\n",
        "    openai_api_key = userdata.get(\"OPEN_AI_KEY\")\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    # Encode image\n",
        "    image_b64 = encode_image(image_path)\n",
        "\n",
        "    # Send request\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",  # or \"gpt-4o\"\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": text_input},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_b64}\"}\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    img_path = \"/content/sample image.jpg\" # Replace with your image path\n",
        "    user_text = \"What is happening in this picture?\"\n",
        "\n",
        "    output = multimodal_agent(img_path, user_text)\n",
        "    print(\"Output:\", output)"
      ]
    }
  ]
}